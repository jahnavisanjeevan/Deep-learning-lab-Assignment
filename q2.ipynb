# ===============================
# Q2: Text-driven Image Segmentation with SAM 2
# ===============================

# --- Install & Imports ---
!pip install git+https://github.com/facebookresearch/segment-anything.git
!pip install opencv-python pillow matplotlib transformers

import torch, cv2, numpy as np, matplotlib.pyplot as plt
from PIL import Image
from segment_anything import SamPredictor, sam_model_registry

# --- Load Image & Prompt ---
img_path = "https://raw.githubusercontent.com/ultralytics/assets/main/data/images/zidane.jpg"
prompt = "person"

image = np.array(Image.open(requests.get(img_path, stream=True).raw))
plt.imshow(image)
plt.title(f"Input Image\nPrompt: {prompt}")
plt.axis('off')
plt.show()

# --- Load SAM model ---
sam_checkpoint = "sam_vit_h_4b8939.pth"  # pretrained SAM weights
sam = sam_model_registry["vit_h"](checkpoint=sam_checkpoint)
predictor = SamPredictor(sam)

# --- Preprocess ---
predictor.set_image(image)

# --- Dummy text â†’ seed (simulate region detection) ---
# In real case: use CLIPSeg / GroundingDINO to get box/points
input_point = np.array([[300, 400]])
input_label = np.array([1])

# --- Predict Mask ---
masks, scores, logits = predictor.predict(point_coords=input_point, point_labels=input_label, multimask_output=True)

# --- Display ---
mask = masks[np.argmax(scores)]
plt.imshow(image)
plt.imshow(mask, alpha=0.5)
plt.title("Segmented Region (SAM 2 Output)")
plt.axis("off")
plt.show()
